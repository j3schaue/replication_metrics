---
title: "Fisherâ€™s Method"
author: "Katie Fitzgerald and Rrita Zejnullahi"
date: "06/20/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Fisher's test

Among the 100 replication studies conducted by the Open Science Collaboration (OSC), 64 found null effects, defined as having a p-value greater than 0.05. Because only 3 of the original studies found null effects, when using the "sign and significance" metric to assess replication, the OSC took this to indicate that there was widespread failure in replication. One possible explanation for this "failure to replicate," however, would be that the replication studies were simply underpowered to detect the true non-zero effects that the original studies found. In general, a finding of a null effect indicates that either the true effect is in fact zero or that it is non-zero but the test was underpowered to detect it. Hoping to determine which of these two scenarios is true of their replication studies, the OSC applied Fisher's method to the set of 64 non-significant p-values to test the null hypothesis that a true zero effect held for each study and that there were no false negatives among them. Their hypothesis for Fisher's test could be formalized as follows, where $\theta_j$ is the treatment effect for study $j$:
\begin{center}
$H_0$: $\theta_1=\theta_2=...=\theta_{64}=0$

$H_a$: at least one $\theta_j\neq 0,\quad j=1,...,64$
\end{center}

Because the OSC conducted Fisher's test only among the 64 replication studies that did not have significant results, and thus $p_j \geq 0.05$ for all $j$, they used the following transformation of Fisher's test statistic  $$X^2=-2\sum_{j=1}^{k} ln(p_j^*)=-2\sum_{j=1}^{k} ln(\frac{p_j-0.05}{0.95}),$$ where under $H_0$, $X^2 \sim \chi^2_{2k}$, where $k$ is the number of studies (in this case $k=64$). Low p-values give a larger test statistic, leading to a rejection of $H_0$.

We believe that Fisher's test is not well suited to assess replication. In the event that you do reject $H_0$, this result only tells you that at least one study was a false negative, but it does not tell you *which* study did in fact have a true non-zero effect. Even still, it provides no information about the size or direction of that non-zero effect and whether or not it replicates the original finding. Presumably, a finding of "no false negatives" would be most informative in assessing replication in this scenario, but this can never be validly concluded from Fisher's method since that would require concluding the null hypothesis. While it is never advised to conduct a test in order to conclude the null hypothesis, this switched framework is especially problematic when the test is underpowered to reject $H_0$ because the Type II error rate will be large. 

Even though the OSC was able to reject their null hypothesis ($X^2=155.83, p=0.048$), we think that in general Fisher's method is underpowered to answer the question at hand. We hypothesize that it requires many and possibly large non-null effects in order to skew the distribution of p-values enough to reject Fisher's null hypothesis. In the scenario where a researcher is combining the results of $k$ studies of the same treatment to test if an overall treatment effect $\Theta=0$, this type of conservative test may be appropriate. In the OSC scenario, however, since the 64 studies are not testing the same treatment effect, and one $\theta_j$ has no bearing on the 63 other $\theta_j$'s, the presumed goal would be to detect if there are *any* false negatives among the replicate studies.  

The true distribution of Fisher's test statistic under the alternative hypothesis is unknown, and therefore the power cannot be calculated exactly. The asymptotic distribution of $X^2$ can be shown to be approximately normal as within study sample sizes go to infinity, but this approximation is only valid for impractical sample sizes and detectable effect sizes.[^1] We therefore turn to simulations to investigate the power of Fisher's method in the OSC scenario. 

[^1]: See Appendix B

For simplicity of interpretation but without loss of generality, we will work with treatment effects on the standardized scale of Cohen's d, defined as $\delta_j=\frac{\theta_j}{\sigma_j}$, where $\theta_j$ is the mean difference between the treatment and control groups in study $j$, and $\sigma_j$ is the equal variance among the treatment and control populations in study $j$. Assuming equal sample sizes in the treatment and control groups within study $j$ (that is, let $n_j^t=n_j^c=n_j$), $\delta_j$ is estimated by $d_j\sim N(\delta_j,\frac{2}{n_j} + \frac{\delta_j^2}{4n_j})$.[^2] Under this framework, Fisher's method can be represented in Cohen's d as testing the hypotheses  

[^2]: Hedges & Olkin (1985). 

\begin{center}
$H_0$: $\delta_1=\delta_2=...=\delta_{64}=0$

$H_a$: at least one $\delta_j\neq 0,\quad j=1,...,64,$
\end{center} 
and the 64 p-values to be summed in Fisher's test statistic can be calculated as $p_j=2(1-\Phi(\frac{|\delta_j|}{\sqrt{\frac{2}{n_j} + \frac{\delta_j^2}{4n_j}}})).$ 

[^3]: See Appendix A.1 for code and details on how the power simulations were conducted. 

In order to consider the power of Fisher's test under a "best-case scenario" in this dataset, we sort the 64 sample sizes and let the non-null effects be from the studies with the largest sample sizes first. That is, if there is just one non-null effect we let it be from the largest study; if there are two non-null effects we let them be from the two largest studies, etc.[^3] This will provide the highest possible power of Fisher's method given these data.[^4] As shown in the first row of Table 1, even when the study with the largest sample size has a large effect, Fisher's method has less than 10% power to detect it. When the non-null effects come from the studies with the largest sample sizes, about half of the studies need to have $\delta=0.2$ in order to achieve approximately 80% power (power=0.8340). 

Table: Power of Fisher's method given large sample sizes for varying $\delta$ and true # of non-null effects (i.e. "Best-case scenario")
```{r, echo=FALSE}
library(knitr)
table <- readRDS("./power_large_n.RDS")
kable(table,format="markdown",digits=4,align=c('l','r','r','r'),col.names=c("# of non-null effects","$\\delta=0.2$","$\\delta=0.5$","$\\delta=0.8$"),caption="Power to reject H0 with large n")
```
[^3]:This provides a "best-case scenario" because large $n_j$'s lead to smaller $p_j$'s, which in turn result in a larger test statistic $X^2$ and greater likelihood of rejecting $H_0$ (i.e. higher power).

[^4]:Note that because we are working with a set of replicate studies which found a p-value greater than 0.05, pairing large effects with very large sample sizes is not realistic, and therefore we begin the simulations with the largest $n_j$ among the studies for which the power is at most 99.99% to detect the given $\delta_j$. The largest sample sizes used for $\delta_j=0.2; 0.5$; and $0.8$ were $n_j=745; 159;$ and $100$ respectively. See Appendix A.2 for further discussion.

We now consider the power of Fisher's method under more general circumstances. Figure 1 plots the power of Fisher's method against the proportion of studies with false negatives, for varying numbers of total studies ($k$), effect sizes ($\delta$), and within-group within-study sample sizes ($n$). 

```{r, echo = FALSE, warning = FALSE, message = FALSE}
###CREATE PLOT####
k10 <- readRDS("./k10_data.RDS")
k50 <- readRDS("./k50_data.RDS")
k100 <- readRDS("./k100_data.RDS")

library(ggplot2)
n_labels <- c('25' = "n=25", '50' = "n=50", '75' = "n=75", '100'= "n=100")
k_labels <- c( '10' = "k=10", '50' = "k=50", '100' = "k=100")

k10$prop <- k10$M/k10$k
k50$prop <- k50$M/k50$k
k100$prop <- k100$M/k100$k

all <- rbind(k10,k50,k100)
plot_all <- ggplot(all, aes(x = prop, y = power, color = factor(delta))) +
  facet_grid(k~n, labeller = labeller(n = n_labels, k = k_labels)) +
  theme(panel.grid.minor = element_blank(),
        axis.ticks = element_blank(), plot.title = element_text(hjust = 0.5)) +
  scale_color_brewer(palette="Dark2")+
  scale_x_continuous(name = "Proportion of false negatives", limits = c(0,1), breaks = seq(0,1,.20)) +
  scale_y_continuous(name = "Power", limits = c(0,1), breaks = seq(0,1,0.1))+
  #geom_point(size = 3) +
  #facet_grid(.~n, labeller = labeller(n = labels)) +
  labs(title = "Power of Fisher's method", color = expression(delta))+
  #geom_smooth(method = "loess", se = FALSE, size = 0.5)
  geom_line()

plot_all
```

For example, in the first pane of the grid, the green curve indicates that when there are 10 studies each having a within-group sample sizes of 25, even when all 10 studies have true small effects, the power of Fisher's method is only around 15%. Note the general pattern: the power of Fisher's method increases as within-group sample size increases (left to right in each row), as the number of studies increases (top to bottom in each column), and as the magnitude of the true effect size increases (indicated by the color of the curve). 

As demonstrated by Figure 1, Fisher's method has a large Type II error rate under many common scenarios in social science research, and we do not recommend it as a valid metric for assessing replication. Failing to reject $H_0$ is likely an artifact of the poor properties of Fisher's method and is not valid evidence to conclude there are no false negatives. As stated previously, even in the event that $H_0$ is rejected, it tells the researcher nothing about *which* study has a non-zero effect or if the size or direction of that effect is consistent with the original study. 

#Appendices

##Appendix A: Fisher's method power simulations

###A.1 Power simulation logic and code
Let there be $m$ false negatives (i.e. $m$ true non-zero effects) among the 64 studies, $m=1,...,64$. Therefore we must draw $m$ p-values from a distribution consistent with the alternative hypothesis. That is, we draw a random variable $d_j$ from a $N(\delta_j,\frac{2}{n_j} + \frac{\delta_j^2}{4n_j})$ distribution, where $\delta_j\neq0$ and compute its p-value. We continue drawing $d_j's$ until we obtain $m$ p-values greater than 0.05 (due to the OSC restriction of only considering replicate studies with non-significant results). We will draw the remaining $64-m$ p-values from a U[0.05,1] distribution and then calculate Fisher's test statistic $X^2=-2\sum_{j=1}^{k} ln(\frac{p_j-0.05}{0.95})$. We run this procedure N times and calculate the simulated power of Fisher's method under these conditions to be $\sum_{q=1}^{N}I_{\{X^2_q > \chi^2_{2k,\alpha}\}}/N,$ where $I$ is the indicator function and $\chi^2_{2k,\alpha}$ is the critical value for Fisher's test with $k$ studies and level $\alpha$. We let N=100,000. The code is given below.

```{r power_sims}
power_sims<-function(N,M,delta,n,k){
  #########################################################################################
  # TAKES: N; number of simulations 
  #        M; vector of number of non-null effects
  #        delta; effect size under alternative hypothesis, on scale of cohen's d
  #        n; vector of treatment/control sample size across studies (total sample size/2)
  #        k; number of studies
  # RETURNS: power of Fisher's test to reject
  # Assumes 2-sided p-values, throws away p-values<=0.05 to match OSC methods
  #########################################################################################
  
  T<-c() #empty list to store Fisher's test statistic 
  Power<-matrix() #empty matrix to store results
  
  for(l in 1:length(M)){
    
    for (i in 1:N){
      
      #print(i) # can uncomment to show progress for lengthy simulations
    
      p0<-runif(k - M[l], 0.05, 1) #draws p-values for the true null effects
      
      p1<-c() #create list to store p-values drawn for non-null effects
      
      for (j in 1:M[l]){ 
        
        p1[j]<-0
        
        while (p1[j] <= 0.05) { #throw away p-values<=0.05
          z[j] <- rnorm(1,delta, sqrt(2/n[j] + delta^2/(4*n[j])))
          p1[j] <- 2*(1-pnorm(abs(z[j])/sqrt(2/n[j] + delta^2/(4*n[j]))))
        }
        
        #print(n[j]) # can uncomment to show progress for lengthy simulations 
        }
      
      #test statistic for Fisher's method, with transformation for truncating p-values
      T[i]<--2*sum(log((p0-0.05)/0.95))-2*sum(log((p1-0.05)/0.95)) 
    
      }
    
    Power[l]<-sum(T > qchisq(0.95, 2*k) )/N
  
    }
  
    return(Power)
}
```

##A.2 "Best-case scenario"" power simulations

As noted in footnote 4 of the text, the largest sample sizes were dropped out of necessity in the "best-case scenario" power simulations presented in Table 1. For example, the largest study had $n_j=384351.5$, which is powered at 100% to detect even a small $\delta=0.2$. Therefore, it would have been impossible for this study to result in a p-value greater than 0.05 if there was a true small effect. Table 3 presents the power of the 12 largest OSC replicate studies (among the subset of 64) to detect a given $\delta$. For example, the twelfth largest OSC study had a within-group sample size of 100 and had 29% power to detect a small effect, 94% power to detect a medium effect, and >99% power to detect a large effect. For each $\delta$, we began the power simulations with the largest $n$ for which the power was at most 0.9999. The largest sample sizes used for $\delta_j=0.2; 0.5$; and $0.8$ therefore were $n_j=745; 159;$ and $100$ respectively. Table 3 presents the resulting sample size vectors used for the "best-case scenario" power simulations.

Table: Power of 12 largest OSC replicate studies to detect $\delta$ given $n$
```{r,echo=FALSE,warning=FALSE,message=FALSE}
#test upper bounds of power (by largest sample size study having false negatives)

#read in OSC data
rpp_data<-read.csv("../data/rpp_data.csv")

#subset only the 64 studies used in Fisher's method and extract sample sizes
data_forFisher<- rpp_data[ !is.na(rpp_data$T_pval_USE..R.) & (rpp_data$T_pval_USE..R.>0.05)  , ]
n<-sort(data_forFisher$N..R.,decreasing=TRUE)/2

#calculate power for OSC replicate studies to detect effects of 0.2, 0.5, and 0.8 given their exact sample size
source("../src/metrics_funs.R")
replicate_power<-round(data.frame(n,power_delta_0.2=power_fun(0.2,2/n),power_delta_0.5=power_fun(0.5,2/n),power_delta_0.8=power_fun(0.8,2/n)),4)

#shorten # of rows to display in table
replicate_power<-replicate_power[1:12,]
replicate_power$Study <- seq(1,12,1)
replicate_power <- replicate_power[,c(5,1,2,3,4)]
library(knitr)
kable(replicate_power,format="markdown",digits=4,align=c('r','r','r','r'),col.names=c("Study","n","$\\delta=0.2$","$\\delta=0.5$","$\\delta=0.8$"),caption="Power of OSC replicate studies to detect $\\delta$ given $\n$")
```
Table: Sample size vectors used in "best case scenario" power simulations for given $\delta$'s
```{r, echo=FALSE,warning=FALSE,message=FALSE}
large_n_used<-readRDS("./large_n_used.RDS")
large_n_table<-rbind(large_n_used[1:13,],c("...","...","..."),large_n_used[64,])
rownames(large_n_table)[14]<-c("...")
kable(large_n_table,format="markdown",digits=1,col.names=c("$\\delta=0.2$","$\\delta=0.5$","$\\delta=0.8$"))

```

##Appendix B: Fisher's method asymptotic results

Under the Neyman and Pearson hypothesis testing framework, the significance level follows a uniform distribution on $\lbrack0,1\rbrack$ when the null hypothesis holds, however, the exact distribution under the alternative hypothesis is unknown. As a consequence of this, we rely on long known results from asymptotic theory. Lambert and Hall have shown that, given the test statistic is asymptotically normal, the one sided P-value follows a lognormal distribution with mean $-nc(\theta)$ and variance $n\tau^2(\theta)$ (1982). The parameter $c(\theta)$ is defined as half the Bahadur slope, given by $-\frac{1}{n}lim_{n\to\infty}logP_n=c(\theta)$, and is the exponential rate at which the significance level converges to zero under the alternative hypothesis. In addition, observe that the variance of the standardized P-value is $\frac {\tau^2(\theta)}{n}$. We can approximate the two sided P-value by doubling the one sided one, which implies that $P_n\sim AlogN\lbrack-\frac{1}{2}nc(\theta), \frac {1}{4}n\tau^2(\theta)\rbrack$, and thus $logP_n\sim AN\lbrack-\frac{1}{2} nc(\theta), \frac{1}{4}n\tau^2(\theta)\rbrack$. Multiplying $logP_n$ by $-2$, we obtain $-2logP_n\sim AN\lbrack nc(\theta), n\tau^2(\theta)\rbrack$. Note that $-2logP_{n_j}$, $j=1,...,k$, are asymptotically independent random variables following identical distributions, therefore under $H_a$ $$X^2_n=-2\sum_{j=1}^{k} logP_{n_j} \sim AN(\sum_{j=1}^{k}n_j c_j(\theta), \sum_{j=1}^{k}n_j \tau^2_j(\theta))$$. 

Examining the behavior of $X^2_n$ using simulated data, we find that the normal approximation is valid if the within study sample size $n_j$ is at least 1500, 250, and 100 for $\theta$ equal to 0.2, 0.5, and 0.8, respectively. Note that $\theta$ in this case can be interpreted as Cohen's d because $\sigma$ is assumed to be 1. When the within study sample sizes are smaller than indicated previously, the distribution of the test statistic is skewed to the right, and therefore the normal approximation is not valid.

```{r,echo=FALSE, message=FALSE, fig.show='hold', fig.align='center'}

p1<-c()
dist_of_X2<-function(delta,n){
  for (j in 1:100000){ 
    p1[j]<-2*(1 - pnorm(abs(rnorm(1, delta, sqrt(2/n + delta^2 / (4*n))))/sqrt(2/n + delta^2 / (4*n))))
  }
  X2<--2*log(p1)
}

d1 <- as.data.frame(dist_of_X2(0.2,700))
colnames(d1)[1]<-"V1"
ggplot(d1, aes(x = V1)) +
  geom_density()

library(latex2exp)
par(mfrow=c(3,2))

hist(dist_of_X2(0.2,700), main=TeX('n=700, $\\theta$=0.2'), xlab=expression(-2*log(p_i)))
x<-seq(0,60,0.1)
curve(dnorm(x, mean = 0.5*700*2*1/4*0.2^2, sd = sqrt(2*700*1/4*0.2^2)), add = TRUE)
hist(dist_of_X2(0.2,1500), main=TeX('n=1500, $\\theta$=0.2'), xlab=expression(-2*log(p_i)))

hist(dist_of_X2(0.5,100), main=TeX('n=100, $\\theta$=0.5'),  xlab=expression(-2*log(p_i)))
hist(dist_of_X2(0.5,250), main=TeX('n=250, $\\theta$=0.5'),  xlab=expression(-2*log(p_i)))

hist(dist_of_X2(0.8,50), main=TeX('n=50, $\\theta$=0.8'),  xlab=expression(-2*log(p_i)))
hist(dist_of_X2(0.8,100), main=TeX('n=100, $\\theta$=0.8'),  xlab=expression(-2*log(p_i)))
```


Now consider the two sample shift problem as an example. Let $Y_{11},Y_{21},...,Y_{n1}$ denote a sample of i.i.d. observations from a normal distribution with mean $\mu$ and standard deviation 1. Let $Y_{12},Y_{22},...,Y_{n2}$ denote a second sample, independent of the first, with i.i.d. observations from a normal $(\mu+\theta, 1)$. For simplicity, suppose that $n_1$ and $n_2$ are equal. We test $H_0:\theta=0$ versus $H_a:\theta\neq0$ by using $\frac{\sqrt{n}(\overline{Y_2}-\overline{Y_1})}{\sqrt{2}}$. According to Lambert and Hall, $c(\theta)=\frac{1}{2}\lambda\overline{\lambda}\theta^2$ and $\tau^2(\theta)=\lambda\overline{\lambda}\theta^2$, where $\lambda$ denotes the fractional sample size, and $\overline{\lambda}=1-\lambda$. Since we are assuming $n_1$ is equal to $n_2$, both $\lambda$ and $\overline{\lambda}$ are $\frac{1}{2}$. Then, if all k studies are testing the same hypothesis, it follows that $X^2_n$ is asymptotically normal with grand mean $\sum_{j=1}^{k}\frac{1}{2}n_j \lambda_j \overline{\lambda_j}\theta_j^2$ and grand variance $\sum_{j=1}^{k}n_j \lambda_j \overline{\lambda_j}\theta_j^2$. Consequently, the asymptotic power is:

$$\begin{aligned}
Power&=Pr(reject \ H_0 \ | \ H_a \ is \ true)\\
&=1-Pr(fail \ to \ reject \ H_0 \ | \ H_a \ is \ true)\\
&=1-Pr(|X^2_n| \ < \ \chi^2_{2k} \ | \ H_a \ is \ true)\\
&=1-\Phi\Big(\frac{\chi^2_{2k}-\sum_{j=1}^{k} \frac{1}{2} n_j\lambda_j \overline{\lambda}_j\theta^2_j}{\sum_{j=1}^{k}n_j\lambda_j \overline{\lambda}_j\theta^2_j}\Big)+\Phi\Big(\frac{-\chi^2_{2k}-\sum_{j=1}^{k} \frac{1}{2} n_j\lambda_j \overline{\lambda}_j\theta^2_j}{\sum_{j=1}^{k}n_j\lambda_j \overline{\lambda}_j\theta^2_j}\Big)\\
&=1-\Phi\Big(\frac{\chi^2_{2k}}{\sum_{j=1}^{k}n_j\lambda_j \overline{\lambda}_j\theta^2_j}-\frac{1}{2}\Big)+\Phi\Big(\frac{-\chi^2_{2k}}{\sum_{j=1}^{k}n_j\lambda_j \overline{\lambda}_j\theta^2_j}-\frac{1}{2}\Big)\\
&=1
\end{aligned}.$$

The following plots of the power function reveal that asymptotic power approaches 1 when the total within study sample size is greater than 600, 100, and 50 for a fixed $\theta$ of 0.2, 0.5, and 0.8, respectively. 

```{r prt 1, echo=FALSE, message=FALSE, fig.width=3, fig.height=5,fig.show='hold',fig.align='center'}
power<-function(n,delta){
  1-pnorm(155.4047, mean=64*(1/2)*n*(1/4)*(delta^2), sqrt(64*n*(1/4)*(delta^2)))+pnorm(-155.4047, mean=64*(1/2)*n*(1/4)*(delta^2), sqrt(64*n*(1/4)*(delta^2)))
}
require(latex2exp)
par(mfrow=c(3,1))
plot(power(1:700,0.2), type='l', xlab='n', ylab='Power', main=TeX('$\\theta$=0.2'))
plot(power(1:700,0.5), type='l', xlab='n', ylab='Power', main=TeX('$\\theta$=0.5'))
plot(power(1:700,0.8), type='l', xlab='n', ylab='Power', main=TeX('$\\theta$=0.8'))
```
