---
title: "Fisherâ€™s Method"
author: "Katie Fitzgerald and Rrita Zejnullahi"
date: "06/20/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Fisher's test

Among the 100 replication studies conducted by the Open Science Collaboration (OSC), 64 found null effects, defined as having a p-value greater than 0.05. The OSC applied Fisher's method to this set of non-significant p-values to test the null hypothesis that a true zero effect held for each study and that there were no false negatives among them. Therefore, their hypothesis for Fisher's test could be formalized as follows, where $\theta_j$ is the treatment effect for study $j$:
\begin{center}
$H_0$: $\theta_1=\theta_2=...=\theta_{64}=0$

$H_a$: at least one $\theta_j\neq 0,\quad j=1,...,64$
\end{center}

Because the OSC conducted Fisher's test only among the 64 replication studies that did not have significant results, and thus $p_j \geq 0.05$ for all $j$, they used the following transformation of Fisher's test statistic  $$X^2=-2\sum_{j=1}^{k} ln(p_j^*)=-2\sum_{j=1}^{k} ln(\frac{p_j-0.05}{0.95}),$$ where under $H_0$, the $p_j^*$'s follow a uniform distribution on [0,1] and thus $X^2 \sim \chi^2_{2k}$, where $k$ is the number of studies (in this case $k=64$). Low p-values give a larger test statistic, leading to a rejection of $H_0$.

We believe that Fisher's test is not well suited to assess replication. In the event that you do reject $H_0$, this result only tells you that at least one study was a false negative, but it does not till you *which* study did in fact have a true effect. Even still, it provides no information about the size or direction of that true non-zero effect and whether or not it replicates the original finding. Presumably, a finding of "no false negatives" would be most informative in assessing replication in this scenario, but this can never be validly concluded from Fisher's method since that would require concluding the null hypothesis. While it is never advised to conduct a test in order to conclude the null hypothesis, this switched framework is especially problematic when the test is underpowered to reject $H_0$ because the Type II error rate will be large. 

Even though the OSC was able to reject their null hypothesis ($X^2=155.83, p=0.048$), we think that in general Fisher's method is underpowered to answer the question at hand. We hypothesize that it requires many and possibly large non-null effects in order to skew the distribution of p-values enough to reject Fisher's null hypothesis. In the scenario where a researcher is combining the results of $k$ studies of the same treatment to test if an overall treatment effect $\Theta=0$, this type of conservative test may be appropriate. In the OSC scenario, however, since the 64 studies are not testing the same treatment effect, and one $\theta_j$ has no bearing on the 63 other $\theta_j's$, the presumed goal would be to detect if there are *any* false negatives among the replicate studies.  

The true distribution of Fisher's test statistic under the alternative hypothesis is unknown, and therefore the power cannot be calculated exactly. The asymptotic distribution of $X^2$ can be shown to be approximately normal, but this approximation is not valid for the sample sizes in the OSC data.[^1] We therefore turn to simulations to investigate the power of Fisher's method in the OSC scenario. 

[^1]: See Appendix B

For simplicity of interpretation but without loss of generality, we will work with the standardized scale of Cohen's d, defined as $\delta_j=\frac{\theta_j}{\sigma_j}$, where $\theta_j$ is the mean difference between the treatment and control groups in study $j$, and $\sigma_j$ is the known and equal variance among the treatment and control populations. Assuming equal sample sizes in the treatment and control groups within study $j$ (that is, let $n_j^t=n_j^c=n_j$), $\delta_j$ is estimated by $d_j\sim N(\delta_j,\frac{2}{n_j})$.[^2] Under this framework, Fisher's method can be represented in Cohen's d as testing the hypotheses  

[^2]:See Appendix A.1 for proof and discussion of simplifying assumptions. 

\begin{center}
$H_0$: $\delta_1=\delta_2=...=\delta_{64}=0$

$H_a$: at least one $\delta_j\neq 0,\quad j=1,...,64,$
\end{center} 
and the 64 p-values to be summed in Fisher's test statistic can be calculated as $p_j=2(1-\Phi(\frac{|d_j|}{\sqrt{2/n_j}})).$ 

We first consider the power of Fisher's method when $n_j=76/2=38$ for all $j$, because 76 is the median sample size in the OSC dataset, and we are assuming equal sample sizes in the treatment and control groups. The results of the power simulations for median sample size are given in Table 1.[^3] Note that 0.2, 0.5, and 0.8 correspond to small, medium, and large effects sizes on the scale of Cohen's d. As shown in the last column of Table 1, we find that there needs to be 10 large effects in order to achieve close to the standard 80% power (power=0.7743). If there is only one large effect, Fisher's test only has approximately 8% power to reject. Even when all 64 studies have small effects, Fisher's method only has 70% power to reject. 

[^3]: See Appendix A.2 for code and details on how the power simulations were conducted. 

Table: Power of Fisher's method given median sample size $(n_j=38)$ for varying $\delta$ and true # of non-null effects
```{r,echo=FALSE}
library(knitr)
table <- readRDS("./power sims table with changing n.RDS")
table_median<-table[,1:4]
#colnames(table_median)[2]<
kable(table_median,format="markdown",digits=4,align=c('l','r','r','r'),col.names=c("# of non-null effects","$\\delta=0.2$","$\\delta=0.5$","$\\delta=0.8$"), caption="Power to reject H0 with median sample size (n=38)")

```

In order to consider the power of Fisher's test under a "best-case scenario" in this dataset, we sort the 64 sample sizes and let the non-null effects be from the studies with the largest sample sizes first. That is, if there is just one non-null effect we let it be from the largest study; if there are two non-null effects we let them be from the two largest studies, etc.[^4] As shown in the first row of Table 2, even when a study with a very large sample size has a large effect, Fisher's method has less than 10% power to detect it. When the non-null effects come from the studies with the largest sample sizes, about half of the studies need to have $\delta=0.2$ in order to achieve approximately 80% power (power=0.8340). 

Table: Power of Fisher's method given large sample sizes for varying $\delta$ and true # of non-null effects (i.e. "Best-case scenario")
```{r, echo=FALSE}
table <- readRDS("./power_large_n.RDS")
kable(table,format="markdown",digits=4,align=c('l','r','r','r'),col.names=c("# of non-null effects","$\\delta=0.2$","$\\delta=0.5$","$\\delta=0.8$"),caption="Power to reject H0 with large n")
```
[^4]:Note that because we are working with a set of replicate studies which found a p-value greater than 0.05, pairing large effects with very large sample sizes is not realistic, and therefore we begin the simulations with the largest $n_j$ among the studies for which the power is at most 99.99% to detect the given $\delta_j$. The largest sample sizes used for $\delta_j=0.2; 0.5$; and $0.8$ were $n_j=745; 159;$ and $100$ respectively. See Appendix A.3 for further discussion.

#Appendices

##Appendix A: Fisher's method power simulations

###A.1 Framework and assumptions

Assume each study $j$ is testing the presence of some treatment effect $\theta_j$, where $j=1,...,k$, and $k$ is the number of studies. Assume equal sample sizes for the treatment and control groups ($n_{tj}=n_{cj}=n_j$). Let $Y_{ij}^t$ and $Y_{ij}^c$ be the observations from the treatment and control groups respectively ($i=1,...,k$, and $j=1,...n_i$), and assume $Y_{jl}^t\sim N(\mu_j +\theta_j, \sigma_j^2)$ and $Y_{jl}^c\sim N(\mu_j, \sigma_j^2)$, where the $\sigma_j^2$'s are known. Note this is not an unreasonbale assumption since this framework is required for both the t-test and the ANOVA test, and the majority of the tests in the OSC subset of 64 replicate studies are of these two types (89%). 

Note then, $\theta_j$ is the difference in means between the treatment and control groups, and its estimate $T_j=\overline{Y_{j.}^t}-\overline{Y_{j.}^c}$ has variance $v_j=\frac{\sigma^2}{n_{cj}} +\frac{\sigma^2}{n_{tj}}=\frac{2\sigma^2}{n_j}.$ For simplicity of interpretation but without loss of generality, we work with the standardized scale of Cohen's d, defined as $\delta_j=\frac{\theta_j}{\sigma}$. Note then that $\delta_j$ is estimated by $d_j=\frac{T_j}{\sigma}=\frac{T_j}{\sqrt{\frac{v_j}{2/n_j}}}=\frac{T_j}{\sqrt{v_j}}\sqrt{{\frac{2}{n_j}}}.$ Since under $H_0$, $T_j\sim N(\theta_j,v_j) \Rightarrow \frac{T_j}{\sqrt{v_j}} \sim N(\frac{\theta_j}{\sqrt{v_j}},1)$ and $\sqrt{\frac{2}{n_j}}$ is a constant, then we have  $Var(d_j)=Var(\frac{T_j}{\sqrt{v_j}}\sqrt{{\frac{2}{n_j}}})=\frac{2}{n_j}$. Therefore, $d_j\sim N(\delta_j,\frac{2}{n_j})$.

###A.2 Power simulation logic and code
Let there be $m$ false negatives (i.e. $m$ true non-zero effects) among the 64 studies, $m=1,...,64$. Therefore we must draw $m$ p-values from a distribution consistent with the alternative hypothesis. That is, we draw a random variable $d_j$ from a $N(\delta_j,\frac{2}{n_j})$ distribution, where $\delta_j\neq0$ and compute its p-value. We continue drawing $d_j's$ until we obtain $m$ p-values greater than 0.05 (due to the OSC restriction of only considering replicate studies with non-significant results). We will draw the remaining $64-m$ p-values from a U[0.05,1] distribution and then calculate Fisher's test statistic $X^2=-2\sum_{j=1}^{k} ln(\frac{p_j-0.05}{0.95})$. We run this procedure N times and calculate the simulated power of Fisher's method under these conditions to be $\sum_{q=1}^{N}I_{\{X^2_q>155.4047\}}/N,$ where $I$ is the indicator function and $\chi^2_{128}=155.4047$ is the critical value for Fisher's test with $k=64$ studies. We let N=100,000. The code is given below.

```{r power_sims}
power_sims<-function(N,M,delta,n){
  ########################################################################################
  # TAKES: N; number of simulations 
  #        M; vector of number of non-null effects (e.g. M=c(1, 2, 3, 4, 5, 32, 64))
  #        delta; effect size under alternative hypothesis, on scale of cohen's d
  #        n; vector of treatment/control sample size across studies (total sample size/2)
  # RETURNS: power of Fisher's test to reject
  # Assumes 2-sided p-values, throws away p-values<=0.05 to match OSC methods
  ########################################################################################
  
  T<-c() #empty list to store Fisher's test statistic 
  Power<-matrix() #empty matrix to store results
  
  for(k in 1:length(M)){
    
    for (i in 1:N){
      
      #print(i) # can uncomment to show progress for lengthy simulations
    
      p0<-runif(64 - M[k], 0.05, 1) #draws p-values for the true null effects
      
      p1<-c() #create list to store p-values drawn for non-null effects
      
      for (j in 1:M[k]){ 
        
        p1[j]<-0
        
        while (p1[j] <= 0.05) { #throw away p-values<=0.05
          p1[j]<-2 * (1 - pnorm(abs(rnorm(1, delta, sqrt(2 / n[j]))) / sqrt(2 / n[j])))
        }
        
        #print(n[j]) # can uncomment to show progress for lengthy simulations 
        }
      
      #test statistic for Fisher's method, with transformation for truncating p-values
      T[i]<--2 * sum(log((p0 - 0.05)/0.95)) - 2 * sum(log((p1 - 0.05) / 0.95)) 
    
      }
    
    Power[k]<-sum(T > 155.4047) / N
  
    }
  
    return(Power)
}
```

##A.3 "Best-case scenario"" power simulations

As noted in footnote 4 of the text, the largest sample sizes were dropped out of necessity in the "best-case scenario" power simulations presented in Table 2. For example, the largest study had $n_j=384351.5$, which is powered at 100% to detect even a small $\delta=0.2$. Therefore, it would have been impossible for this study to result in a p-value greater than 0.05 if there was a true non-null effect. Table 3 presents the power of the OSC replicate studies to detect a given $\delta$ using the true sample sizes for the 64 studies. For each $\delta$, we began the power simulations with the largest $n$ for which the power was at most 0.9999. The largest sample sizes used for $\delta_j=0.2; 0.5$; and $0.8$ therefore were $n_j=745; 159;$ and $100$ respectively. Table 4 presents the sample size vectors used for the "best-case scenario" power simulations.

Table: Power of OSC replicate studies to detect $\delta$ given $n$
```{r,echo=FALSE,warning=FALSE,message=FALSE}
#test upper bounds of power (by largest sample size study having false negatives)

#read in OSC data
rpp_data<-read.csv("../data/rpp_data.csv")

#subset only the 64 studies used in Fisher's method and extract sample sizes
data_forFisher<- rpp_data[ !is.na(rpp_data$T_pval_USE..R.) & (rpp_data$T_pval_USE..R.>0.05)  , ]
n<-sort(data_forFisher$N..R.,decreasing=TRUE)/2

#calculate power for OSC replicate studies to detect effects of 0.2, 0.5, and 0.8 given their exact sample size
source("../src/metrics_funs.R")
replicate_power<-round(data.frame(n,power_delta_0.2=power_fun(0.2,2/n),power_delta_0.5=power_fun(0.5,2/n),power_delta_0.8=power_fun(0.8,2/n)),4)

#shorten # of rows to display in table
replicate_power<-rbind(replicate_power[1:12,],c("...","...","...","..."))
library(knitr)
kable(replicate_power,format="markdown",digits=4,align=c('r','r','r','r'),col.names=c("n","$\\delta=0.2$","$\\delta=0.5$","$\\delta=0.8$"),caption="Power of OSC replicate studies to detect $\\delta$ given $\n$")
```
Table: Sample size vectors used in "best case scenario" power simulations for given $\delta$'s
```{r, echo=FALSE,warning=FALSE,message=FALSE}
large_n_used<-readRDS("./large_n_used.RDS")
large_n_table<-rbind(large_n_used[1:13,],c("...","...","..."),large_n_used[64,])
rownames(large_n_table)[14]<-c("...")
kable(large_n_table,format="markdown",digits=1,col.names=c("$\\delta=0.2$","$\\delta=0.5$","$\\delta=0.8$"))

```

##Appendix B: Fisher's method asymptotic results

Under the Neyman and Pearson hypothesis testing framework, the significance level follows a uniform distribution on $\lbrack0,1\rbrack$ when the null hypothesis holds, however, the exact distribution under the alternative hypothesis is unknown. As a consequence of this, we rely on long known results from asymptotic theory. Lambert and Hall have shown that, given the test statistic is asymptotically normal, the one sided P-value follows a lognormal distribution with mean $-nc(\theta)$ and variance $n\tau^2(\theta)$ (1982). The parameter $c(\theta)$ is defined as half the Bahadur slope, given by $-\frac{1}{n}lim_{n\to\infty}logP_n=c(\theta)$, and is the exponential rate at which the significance level converges to zero under the alternative hypothesis. In addition, observe that the variance of the standardized P-value is $\frac {\tau^2(\theta)}{n}$. We can approximate the two sided P-value by doubling the one sided one, which implies that $P_n\sim AlogN\lbrack-\frac{1}{2}nc(\theta), \frac {1}{4}n\tau^2(\theta)\rbrack$, and thus $logP_n\sim AN\lbrack-\frac{1}{2} nc(\theta), \frac{1}{4}n\tau^2(\theta)\rbrack$. Multiplying $logP_n$ by $-2$, we obtain $-2logP_n\sim AN\lbrack nc(\theta), n\tau^2(\theta)\rbrack$. Note that $-2logP_{n_j}$, $j=1,...,k$, are asymptotically independent random variables following identical distributions, therefore under $H_a$ $$X^2_n=-2\sum_{j=1}^{k} logP_{n_j} \sim AN(\sum_{j=1}^{k}n_j c_j(\theta), \sum_{j=1}^{k}n_j \tau^2_j(\theta))$$. 

Examining the behavior of $X^2_n$ using simulated data, we find that the normal approximation is valid if the within study sample size $n_j$ is at least 1500, 250, and 100 for $\theta$ equal to 0.2, 0.5, and 0.8, respectively. Note that $\theta$ in this case can be interpreted as Cohen's d because $\sigma$ is assumed to be 1. When the within study sample sizes are smaller than indicated previously, the distribution of the test statistic is skewed to the right, and therefore the normal approximation is not valid.

```{r,echo=FALSE, message=FALSE, fig.show='hold', fig.align='center'}

p1<-c()
dist_of_X2<-function(delta,n){
  for (j in 1:100000){ 
    p1[j]<-2*(1 - pnorm(abs(rnorm(1, delta, sqrt(2/n)))/sqrt(2/n)))
  }
  X2<--2*log(p1)
}

library(latex2exp)
par(mfrow=c(3,2))

hist(dist_of_X2(0.2,700), main=TeX('n=700, $\\theta$=0.2'), xlab=expression('X'^2))
hist(dist_of_X2(0.2,1500), main=TeX('n=1500, $\\theta$=0.2'), xlab=expression('X'^2))

hist(dist_of_X2(0.5,100), main=TeX('n=100, $\\theta$=0.5'),  xlab=expression('X'^2))
hist(dist_of_X2(0.5,250), main=TeX('n=250, $\\theta$=0.5'),  xlab=expression('X'^2))

hist(dist_of_X2(0.8,50), main=TeX('n=50, $\\theta$=0.8'),  xlab=expression('X'^2))
hist(dist_of_X2(0.8,100), main=TeX('n=100, $\\theta$=0.8'),  xlab=expression('X'^2))
```


Now consider the two sample shift problem as an example. Let $Y_{11},Y_{21},...,Y_{n1}$ denote a sample of i.i.d. observations from a normal distribution with mean $\mu$ and standard deviation 1. Let $Y_{12},Y_{22},...,Y_{n2}$ denote a second sample, independent of the first, with i.i.d. observations from a normal $(\mu+\theta, 1)$. For simplicity, suppose that $n_1$ and $n_2$ are equal. We test $H_0:\theta=0$ versus $H_a:\theta\neq0$ by using $\frac{\sqrt{n}(\overline{Y_2}-\overline{Y_1})}{\sqrt{2}}$. According to Lambert and Hall, $c(\theta)=\frac{1}{2}\lambda\overline{\lambda}\theta^2$ and $\tau^2(\theta)=\lambda\overline{\lambda}\theta^2$, where $\lambda$ denotes the fractional sample size, and $\overline{\lambda}=1-\lambda$. Since we are assuming $n_1$ is equal to $n_2$, both $\lambda$ and $\overline{\lambda}$ are $\frac{1}{2}$. Then, if all k studies are testing the same hypothesis, it follows that $X^2_n$ is asymptotically normal with grand mean $\sum_{j=1}^{k}\frac{1}{2}n_j \lambda_j \overline{\lambda_j}\theta_j^2$ and grand variance $\sum_{j=1}^{k}n_j \lambda_j \overline{\lambda_j}\theta_j^2$. Consequently, the asymptotic power is:

$$\begin{aligned}
Power&=Pr(reject \ H_0 \ | \ H_a \ is \ true)\\
&=1-Pr(fail \ to \ reject \ H_0 \ | \ H_a \ is \ true)\\
&=1-Pr(|X^2_n| \ < \ \chi^2_{2k} \ | \ H_a \ is \ true)\\
&=1-\Phi\Big(\frac{\chi^2_{2k}-\sum_{j=1}^{k} \frac{1}{2} n_j\lambda_j \overline{\lambda}_j\theta^2_j}{\sum_{j=1}^{k}n_j\lambda_j \overline{\lambda}_j\theta^2_j}\Big)+\Phi\Big(\frac{-\chi^2_{2k}-\sum_{j=1}^{k} \frac{1}{2} n_j\lambda_j \overline{\lambda}_j\theta^2_j}{\sum_{j=1}^{k}n_j\lambda_j \overline{\lambda}_j\theta^2_j}\Big)\\
&=1-\Phi\Big(\frac{\chi^2_{2k}}{\sum_{j=1}^{k}n_j\lambda_j \overline{\lambda}_j\theta^2_j}-\frac{1}{2}\Big)+\Phi\Big(\frac{-\chi^2_{2k}}{\sum_{j=1}^{k}n_j\lambda_j \overline{\lambda}_j\theta^2_j}-\frac{1}{2}\Big)\\
&=1
\end{aligned}.$$

The following plots of the power function reveal that asymptotic power approaches 1 when the total within study sample size is greater than 600, 100, and 50 for a fixed $\theta$ of 0.2, 0.5, and 0.8, respectively. 

```{r prt 1, echo=FALSE, message=FALSE, fig.width=3, fig.height=5,fig.show='hold',fig.align='center'}
power<-function(n,delta){
  1-pnorm(155.4047, mean=64*(1/2)*n*(1/4)*(delta^2), sqrt(64*n*(1/4)*(delta^2)))+pnorm(-155.4047, mean=64*(1/2)*n*(1/4)*(delta^2), sqrt(64*n*(1/4)*(delta^2)))
}
require(latex2exp)
par(mfrow=c(3,1))
plot(power(1:700,0.2), type='l', xlab='n', ylab='Power', main=TeX('$\\theta$=0.2'))
plot(power(1:700,0.5), type='l', xlab='n', ylab='Power', main=TeX('$\\theta$=0.5'))
plot(power(1:700,0.8), type='l', xlab='n', ylab='Power', main=TeX('$\\theta$=0.8'))
```
