---
title: "Fisherâ€™s Method"
author: "Katie Fitzgerald"
date: "05/03/2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#setwd("~/Desktop/NU/Replication/replication_metrics")
```

## Fisher's test

Among the 100 replication studies conducted by the Open Science Collaboration (OSC), 64 of them found null effects, defined as having a p-value greater than 0.05. The OSC authors applied Fisher's method to this set of non-significant p-values to test the hypothesis that these 64 replication studies did in fact have "no evidential value." That is, they tested whether the null hypothesis of zero effect held for each study and that there were no false negatives among them. Therefore, their hypothesis for the Fisher's test could be formalized as follows, where $\theta_i$ is the treatment effect for study $i$:
\begin{center}
$H_0$: $\theta_1=\theta_2=...=\theta_{64}=0$

$H_a$: at least one $\theta_i\neq 0,\quad i=1,...,64$
\end{center}
Often, Fisher's method is used to combine the results of several independent tests on the same overall hypothesis that one $\Theta=0$. For example, if researchers are interested in whether one particular treatment has a true effect, they could conduct *k* studies and pool the results to give an indication if the true $\Theta=0$. In that scenario, if the null hypothesis for each of the *k* studies is true (i.e. $H_0$ above holds), the p-values follow a uniform distribution on [0,1]. The test statistic for Fisher's method is $$X^2=-2\sum_{i=0}^{k} ln(p_i),$$ which follows a $\chi^2$ distribution with $2k$ degrees of freedom when $H_0$ is true. Low p-values give a larger test statistic, leading to a rejection that $\Theta=0$.

In the OSC scenario, however, the authors are conducting Fisher's test only among the 64 replication studies that did not have significant results. By design then, the p-values have a U[0.05,1] distribution. The OSC makes an appropriate transformation, $p_i^*=\frac{p_i-0.05}{0.95},$ so that $p_i^*\sim U[0,1]$ and Fisher's method can be applied. Similarly, the transformed test statistic is  $$X^2=-2\sum_{i=0}^{k} ln(p_i^*)=-2\sum_{i=0}^{k} ln(\frac{p_i-0.05}{0.95}),$$ which follows a $\chi^2$ distribution with $2k$ degrees of freedom under $H_0$. 

Among the 64 non-significant replicate p-values, the OSC finds a test statistic of $X^2=155.83$, which falls just beyond the critical value $\chi^2_{128}=155.40$ and therefore is significant with $p=0.048$. The authors acknowledge this suggests there is at least one replication finding that could be a false negative, but they conclude "nonetheless, the wide distribution of p-values suggests against insufficient power as the only explanation of failures to replicate." This statement seems to indicate the authors are looking for evidence that there are no false negatives among the non-significant replication studies, which is their null hypothesis. 

While it is never advised to conduct a test in order to conclude the null hypothesis, this switched framework is especially problematic when the test is underpowered to reject $H_0$ because the Type II error rate will be large. Even though the authors were able to reject their null hypothesis, we think that in general Fisher's method might be underpowered to answer the question at hand, so we want to explore this more rigorously. The intuition is that if there is just one $\theta_i\neq0$, and the remaining $k-1$ studies have p-values from a U[0,1] distribution, the one non-null effect will need to be very large to skew the distribution enough to look differently from U[0,1]. Or alternatively, there would need to be several non-null effects to skew the distribution to a detectable degree. In the scenario where a researcher is combining the results of $k$ studies of the same treatment to determine if an overall $\Theta=0$, this type of conservative test may be appropriate. In the OSC scenario, however, since the 64 studies are not testing the same treatment effect, and one $\theta_i$ has no bearing on the 63 other $\theta_i's$, the presumed goal would be to detect if there are *any* false negatives among the replicate studies. We will use power simulations to determine if the intuition holds that it would take many and possibly large non-null effects for Fisher's test to reject $H_0$. 

We make a few simplifying assumptions about the framework of these 64 studies in order to proceed. Assume each study $i$ is testing the presence of some treatment effect $\theta_i$ and has equal sample sizes for the treatment and control groups ($n_{ti}=n_{ci}=n_i$). Let $Y_{ij}^t$ and $Y_{ij}^c$ be the observations from the treatment and control groups respectively ($j=1,...n_i$), and assume $Y_{ij}^t\sim N(\mu_i +\theta_i, \sigma^2)$ and $Y_{ij}^c\sim N(\mu_i, \sigma^2)$, where $\sigma^2$ is known. Note this is not an unreasonbale assumption since this framework is required for both the t-test and the ANOVA test, and the majority of the tests in this subset of 64 replicate studies are of these two types (89%). 

Note then, $\theta_i$ is the difference in means between the treatment and control groups, and its estimate $T_i=\overline{Y_{i.}^t}-\overline{Y_{i.}^c}$ has variance $v_i=\frac{\sigma^2}{n_{ci}} +\frac{\sigma^2}{n_{ti}}=\frac{2\sigma^2}{n_i}.$ For simplicity of interpretation but without loss of generality, we will work with the standardized scale of Cohen's d, defined as $\delta_i=\frac{\theta_i}{\sigma}$. Note then that $\delta_i$ is estimated by $d_i=\frac{T_i}{\sigma}=\frac{T_i}{\sqrt{\frac{v_i}{2/n_i}}}=\frac{T_i}{\sqrt{v_i}}\sqrt{{\frac{2}{n_i}}}.$ Since under $H_0$, $T_i\sim N(\theta_i,v_i) \Rightarrow \frac{T_i}{\sqrt{v_i}} \sim N(\frac{\theta_i}{\sqrt{v_i}},1)$ and $\sqrt{\frac{2}{n_i}}$ is a constant, then we have  $Var(d_i)=Var(\frac{T_i}{\sqrt{v_i}}\sqrt{{\frac{2}{n_i}}})=\frac{2}{n_i}$. Therefore, $d_i\sim N(\delta_i,\frac{2}{n_i})$.

The hypotheses of each of the 64 studies can be represented on the scale of Cohen's d as follows: $$H_{0i}: \delta_{i}=0$$ $$H_{ai}: \delta_{i}\neq0,$$ and the p-value of this test can therefore be calculated as $2(1-\Phi(\frac{|d_i|}{\sqrt{2/n_i}})).$

In our simulations, we will assume that there are $m$ false negatives among the 64 studies, $m=1,...64$. Therefore we must draw $m$ p-values from a distribution consistent with the alternative hypothesis. That is, we draw a random variable $d_i$ from a $N(\delta_i,\frac{2}{n_i})$ distribution, where $\delta_i\neq0$ and compute its p-value. We continue drawing $d_i's$ until we obtain $m$ p-values greater than 0.05 (due to the restriction in the OSC scenario of only considering replicate studies with non-significant results). We will draw the remaining $64-m$ p-values from a U[0.05,1] distribution and then calculate Fisher's test statistic $X^2=-2\sum_{i=0}^{k} ln(\frac{p_i-0.05}{0.95})$. We run this procedure N times and calculate the simulated power of Fisher's method under these conditions to be $\sum_{l=0}^{N}I_{\{X^2_l>155.4047\}}/N,$ where $I$ is the indicator function and $\chi^2_{128}=155.4047$ is the critical value for Fisher's test with $k=64$ studies. 

We first consider the case when $n_i=76/2=38$, because 76 is the median sample size in the OSC dataset, and we are assuming equal sample sizes in the treatment and control groups. The results of the power simulations for median sample size are given below. Note that 0.2, 0.5, and 0.8 correspond to small, medium, and large effects sizes on the scale of Cohen's d. We find that there needs to be 10 large effects $\delta_i=0.8$ in order to achieve close to the standard 80% power (power=0.77426). If there is only one large effect, Fisher's test only has approximately 8% power to reject. Even when all 64 studies have small effects, there is only 70% power.  
```{r,echo=FALSE}
library(knitr)
table <- readRDS("~/Desktop/NU/Replication/replication_metrics/Fishers_method/power sims table with changing n.RDS")
table_median<-table[,1:4]
#colnames(table_median)[2]<
kable(table_median,format="markdown",digits=4,align=c('l','r','r','r'),col.names=c("# of non-null effects","$\\delta=0.2$","$\\delta=0.5$","$\\delta=0.8$"),caption="Power to reject H0 with median sample size (n=38)")

```

In order to consider the power of Fisher's test under a "best-case scenario" in this dataset, we sort the 64 sample sizes and let the non-null effects be from the studies with the largest sample sizes first. That is, if $m=1$ we let the non-null effect be from the largest study, if $m=2$ we let the non-null effects be from the two largest studies, etc. Note, however that because we are working with a set of replicate studies which found a p-value greater than 0.05, pairing large effects with very large sample sizes is not realistic. For example, the largest study has $n_i=768703/2=384351.5$ which has 100% power to detect a large effect of $\delta_i=0.8$ (see tableXX in Appendix). In other words, it would not have been possible to obtain a p-value less than 0.05 with this sample size and true $\delta_i=0.8$. We therefore take the largest $n_i$ among the studies for which the power is at most 99.99% to detect the given $\delta_i$. TableXX gives the largest sample sizes used for each $\delta$.

$\delta$ | $n$ | Power of OSC replicate to detect
---------|-----|---------------------------------
0.2      | 745 | 0.9713
0.5      | 159 | 0.9938
0.8      | 100 | 0.9999

See TableXX in the Appendix for the full sample size vectors that were used to conduct the "best-case scenario" power simulations.

As shown in TableXX, even when a study with a very large sample size has a large effect, Fisher's method has less than 10% power to detect it. When the non-null effects come from the studies with the largest sample sizes, about half of the studies need to have $\delta=0.2$ in order to achieve approximately 80% power (power=0.8340)

```{r, echo=FALSE}
table <- readRDS("~/Desktop/NU/Replication/replication_metrics/Fishers_method/power_large_n.RDS")
kable(table,format="markdown",digits=4,align=c('l','r','r','r'),col.names=c("# of non-null effects","$\\delta=0.2$","$\\delta=0.5$","$\\delta=0.8$"),caption="Power to reject H0 with large n")
```

##Asymptotic power
If all 64 have 0.2, need n=278 (total sample size) for 80% power, n=45 for d=0.5, n=18 for d=0.8. 

##Appendix
```{r,echo=FALSE,warning=FALSE,message=FALSE}
#test upper bounds of power (by largest sample size study having false negatives)
rpp_data<-read.csv("~/Desktop/NU/Replication/replication_metrics/data/rpp_data.csv")
data_forFisher<- rpp_data[ !is.na(rpp_data$T_pval_USE..R.) & (rpp_data$T_pval_USE..R.>0.05)  , ]
n<-sort(data_forFisher$N..R.,decreasing=TRUE)/2

source("~/Desktop/NU/Replication/replication_metrics/src/metrics_funs.R")
replicate_power<-data.frame(n,power_delta_0.2=power_fun(0.2,2/n),power_delta_0.5=power_fun(0.5,2/n),power_delta_0.8=power_fun(0.8,2/n))
library(knitr)
kable(replicate_power,format="markdown",digits=4,align=c('r','r','r','r'),col.names=c("n","d=0.2","d=0.5","d=0.8"),caption="Power to reject H0 with median sample size (n=38)")

large_n_used<-readRDS("~/Desktop/NU/Replication/replication_metrics/Fishers_method/large_n_used.RDS")
kable(large_n_used,format="markdown",digits=1,col.names=c("d=0.2","d=0.5","d=0.8"))

```